{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6a72f3f",
   "metadata": {},
   "source": [
    "# PubMed Open Access Alzheimer's Disease Articles: Query and NLP Augmented Parsing Example\n",
    "\n",
    "This notebook demonstrates how to use the `PubMedQueryService`, `PubMedAnalyzer`, and `PMCFullTextParser` classes to:\n",
    "- Search PubMed for open access articles related to Alzheimer's disease (MeSH term) from the past 10 years\n",
    "- Fetch and summarize article metadata\n",
    "- Filter articles by keywords\n",
    "- Retrieve full text from PMC\n",
    "- Parse and extract tables, sections, and figures from full text XML\n",
    "- Analyze and visualize article metadata distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c5ef15",
   "metadata": {},
   "source": [
    "## 1. Initialize environment\n",
    "\n",
    "* Pull NCBI API Key from .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b358596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set your email and API key for NCBI from environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "EMAIL = os.environ.get(\"EMAIL\")  # e.g., 'your.email@example.com'\n",
    "API_KEY = os.environ.get(\"NCBI_API_KEY\")  # e.g., 'your_ncbi_api_key'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcde273a",
   "metadata": {},
   "source": [
    "## 2. Initialize PubMedQueryService\n",
    "\n",
    "Create an instance of PubMedQueryService. Optionally, provide your email and NCBI API key for higher rate limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c23f9d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from niagads.pubmed.services import PubMedQueryService\n",
    "\n",
    "pubmed_service = PubMedQueryService(email=EMAIL, api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b47b07",
   "metadata": {},
   "source": [
    "## 3. Search PubMed for Articles (Current Year Only)\n",
    "\n",
    "Search for open access PubMed articles with the MeSH term 'Alzheimer Disease' for the current year only. This step fetches all PMIDs for the current year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "983ea12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025: 96 articles found\n",
      "Total PMIDs found: 96 (should match count: 96)\n",
      "Total PMIDs found: 96 (should match count: 96)\n"
     ]
    }
   ],
   "source": [
    "from niagads.pubmed.services import PubMedQueryFilters\n",
    "\n",
    "# Search for open access Alzheimer's disease genetics articles (MeSH) for the current year only\n",
    "mesh_term = [\"Alzheimer Disease\", \"Genetics\"]\n",
    "current_year = 2025\n",
    "\n",
    "# Get article count for the current year\n",
    "count = await pubmed_service.find_articles(\n",
    "    filters=PubMedQueryFilters(\n",
    "        keyword=None,\n",
    "        year=current_year,\n",
    "        mesh_term=mesh_term,\n",
    "        open_access_only=True\n",
    "    ),\n",
    "    counts_only=True\n",
    ")\n",
    "print(f\"{current_year}: {count} articles found\")\n",
    "\n",
    "# Fetch all PMIDs for the current year (if any)\n",
    "if count > 0:\n",
    "    all_pmids = await pubmed_service.find_articles(\n",
    "        filters=PubMedQueryFilters(\n",
    "            keyword=None,\n",
    "            year=current_year,\n",
    "            mesh_term=mesh_term,\n",
    "            open_access_only=True\n",
    "        ),\n",
    "        max_results=count,\n",
    "        ids_only=True\n",
    "    )\n",
    "    print(f\"Total PMIDs found: {len(all_pmids)} (should match count: {count})\")\n",
    "else:\n",
    "    all_pmids = []\n",
    "    print(\"No articles found for the current year.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe5a20b",
   "metadata": {},
   "source": [
    "## 4. Fetch Article Metadata\n",
    "\n",
    "Fetch metadata (title, abstract, authors, etc.) for the PMIDs found in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be340e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched metadata for 20 articles.\n",
      "PMID: 40864721\n",
      "Title: Gut-brain nexus: Mapping multimodal links to neurodegeneration at biobank scale.\n",
      "Year: 2025\n",
      "Journal: Science advances\n",
      "Authors: ['Shafieinouri', 'Hong', 'Lee', 'Grant', 'Khani', 'Dadu', 'Schumacher Schuh', 'Makarious', 'Sandon', 'Simmonds', 'Iwaki', 'Hill', 'Blauwendraat', 'Escott-Price', 'Qi', 'Noyce', 'Reyes-Palomares', 'Leonard', 'Tansey', 'Faghri', 'Singleton', 'Nalls', 'Levine', 'Bandres-Ciga']\n",
      "Abstract: Alzheimer's disease (AD) and Parkinson's disease (PD) are influenced by genetic and environmental factors. We conducted a biobank-scale study to (i) identify endocrine, nutritional, metabolic, and dig...\n",
      "\n",
      "PMID: 40862516\n",
      "Title: UnCOT-AD: Unpaired Cross-Omics Translation Enables Multi-Omics Integration for Alzheimer's Disease Prediction.\n",
      "Year: 2025\n",
      "Journal: Briefings in bioinformatics\n",
      "Authors: ['Abir', 'Dip', 'Zhang']\n",
      "Abstract: Alzheimer's Disease (AD) is a progressive neurodegenerative disorder, posing a growing public health challenge. Traditional machine learning models for AD prediction have relied on single omics data o...\n",
      "\n",
      "PMID: 40879647\n",
      "Title: Memantine Administration Enhances Glutamatergic and GABAergic Pathways in the Human Hippocampus of Alzheimer's Disease Patients.\n",
      "Year: 2025\n",
      "Journal: Proteomics\n",
      "Authors: ['Fabrik', 'Kupcik', 'Fabrikova', 'Chvojkova', 'Holubova', 'Hakenova', 'Horak', 'Soukup', 'Manethova', 'Rusina', 'Matej', 'Ryska', 'Soukup']\n",
      "Abstract: One of the traditional treatments in Alzheimer's disease (AD) is administration of memantine, the NMDA receptor antagonist. However, the molecular mechanism of the complex memantine action and the imp...\n",
      "\n",
      "PMID: 40810263\n",
      "Title: Plasma proteomic analysis identifies proteins and pathways related to Alzheimer's risk.\n",
      "Year: 2025\n",
      "Journal: Alzheimer's & dementia : the journal of the Alzheimer's Association\n",
      "Authors: ['Huang', 'Liu', 'Park', 'Chaudhuri', 'Kuchenbecker', 'Carrasquillo', 'Ertekin-Taner', 'Bice', 'Zetterberg', 'Blennow', 'Russ', 'Dage', 'Nudelman', 'Cruchaga', 'Brosch', 'Farlow', 'Clark', 'Mathew', 'Unverzagt', 'Gao', 'Wang', 'Apostolova', 'Wilcock', 'Foroud', 'Risacher', 'Saykin', 'Nho']\n",
      "Abstract: We investigated associations of plasma proteins with blood-based amyloid/tau/neurodegeneration/inflammation (A/T/N/I) biomarkers for Alzheimer's disease (AD)....\n",
      "\n",
      "PMID: 40770318\n",
      "Title: Sleep quality, APOE ε4, and Alzheimer's disease: associations from two prospective cohort studies and mechanisms by plasma proteomic analysis.\n",
      "Year: 2025\n",
      "Journal: BMC medicine\n",
      "Authors: ['Huang', 'Tan', 'Tan', 'Zhang', 'Lin', 'Zou', 'Lin', 'Zhang', 'Hao', 'Zhang', None, 'Xu']\n",
      "Abstract: The relationship between sleep quality and Alzheimer's disease (AD), and its interaction with genetic susceptibility, remains unclear. Our study explores the complex association between sleep quality ...\n",
      "\n",
      "PMID: 40717652\n",
      "Title: Novel set of plasma proteins classifies Alzheimer's dementia in African American individuals with high accuracy.\n",
      "Year: 2025\n",
      "Journal: Alzheimer's & dementia : the journal of the Alzheimer's Association\n",
      "Authors: ['Kuchenbecker', 'Thompson', 'Hurst', 'Huang', 'Heckman', 'Reddy', 'Nguyen', 'Casellas', 'Sotelo', 'Reddy', 'Opdenbosch', 'Tsai', 'Saykin', 'Lucas', 'Willis', 'Day', 'Ramanan', 'Graff-Radford', 'Ertekin-Taner', 'Nho', 'Kalari', 'Carrasquillo']\n",
      "Abstract: African American (AA) individuals are underrepresented in biomarker studies for Alzheimer's disease (AD). Biomarkers that reflect the heterogeneity of AD and achieve the greatest accuracy across popul...\n",
      "\n",
      "PMID: 40696469\n",
      "Title: CHIT1 and DDAH1 levels relate to amyloid-related imaging abnormalities risk profile in Alzheimer's disease patients.\n",
      "Year: 2025\n",
      "Journal: Alzheimer's research & therapy\n",
      "Authors: ['Oosthoek', 'Vijverberg', 'Blujdea', 'Veld', 'Avilés', 'Zsadanyi', 'Hok-A-Hin', 'Visser', 'van der Flier', 'Barkhof', 'Del Campo', 'Schut', 'Bejanin', 'Alcolea', 'Teunissen', 'Vermunt']\n",
      "Abstract: Amyloid-related imaging abnormalities (ARIA) are a common and potentially dangerous side effect in anti-amyloid therapies, creating a need for tools to assess ARIA risk. Several patient factors have b...\n",
      "\n",
      "PMID: 40665052\n",
      "Title: Shared and disease-specific pathways in frontotemporal dementia and Alzheimer's and Parkinson's diseases.\n",
      "Year: 2025\n",
      "Journal: Nature medicine\n",
      "Authors: ['Ali', 'Erabadda', 'Chen', 'Xu', 'Gong', 'Liu', 'Pichet Binette', 'Timsina', 'Western', 'Yang', 'Heo', 'Vogel', 'Tijms', 'Krish', 'Imam', None, 'Hansson', 'Winchester', 'Cruchaga']\n",
      "Abstract: Neurodegenerative diseases (NDs), such as Alzheimer's disease (AD), Parkinson's disease (PD) and frontotemporal dementia (FTD), exhibit distinct yet overlapping pathological mechanisms. Leveraging lar...\n",
      "\n",
      "PMID: 40665049\n",
      "Title: APOE ε4 carriers share immune-related proteomic changes across neurodegenerative diseases.\n",
      "Year: 2025\n",
      "Journal: Nature medicine\n",
      "Authors: ['Shvetcov', 'Johnson', 'Winchester', 'Walker', 'Wilkins', 'Thompson', 'Rothstein', 'Krish', 'Imam', None, 'Burns', 'Swerdlow', 'Slawson', 'Finney']\n",
      "Abstract: The APOE ε4 genetic variant is the strongest genetic risk factor for late-onset Alzheimer's disease (AD) and is increasingly being implicated in other neurodegenerative diseases. Using the Global Neur...\n",
      "\n",
      "PMID: 40665048\n",
      "Title: The Global Neurodegeneration Proteomics Consortium: biomarker and drug target discovery for common neurodegenerative diseases and aging.\n",
      "Year: 2025\n",
      "Journal: Nature medicine\n",
      "Authors: ['Imam', 'Saloner', 'Vogel', 'Krish', 'Abdel-Azim', 'Ali', 'An', 'Anastasi', 'Bennett', 'Pichet Binette', 'Boxer', 'Bringmann', 'Burns', 'Cruchaga', 'Dage', 'Farinas', 'Ferrucci', 'Finney', 'Frasier', 'Hansson', 'Hohman', 'Johnson', 'Kivimaki', 'Korologou-Linden', 'Ruiz Laza', 'Levey', 'Liepelt-Scarfone', 'Lu', 'Mattsson-Carlgren', 'Middleton', 'Nho', 'Oh', 'Petersen', 'Reiman', 'Robinson', 'Rothstein', 'Saykin', 'Shvetcov', 'Slawson', 'Smets', 'Suárez-Calvet', 'Tijms', 'Timmers', 'Vieira', 'Vilor-Tejedor', 'Visser', 'Walker', 'Winchester', 'Wyss-Coray', 'Yang', 'Bose', 'Lovestone', None]\n",
      "Abstract: More than 57 million people globally suffer from neurodegenerative diseases, a figure expected to double every 20 years. Despite this growing burden, there are currently no cures, and treatment option...\n",
      "\n",
      "PMID: 40660303\n",
      "Title: Proteomic landscape of Alzheimer's disease: emerging technologies, advances and insights (2021 - 2025).\n",
      "Year: 2025\n",
      "Journal: Molecular neurodegeneration\n",
      "Authors: ['Yarbro', 'Shrestha', 'Wang', 'Zhang', 'Zaman', 'Chu', 'Wang', 'Yu', 'Peng']\n",
      "Abstract: The advancements of proteomics technologies are shaping Alzheimer's disease (AD) research, revealing new molecular insights and improving biomarker discovery. Here, we summarize major AD proteomics st...\n",
      "\n",
      "PMID: 40653809\n",
      "Title: Label-Free Proteomic Profiling of the dvls2 (CL2006) Caenorhabditis elegans Alzheimer's Disease (AD) Model Reveals Conserved Molecular Signatures Shared With the Human AD Brain.\n",
      "Year: 2025\n",
      "Journal: Journal of neurochemistry\n",
      "Authors: ['Bezerra', 'Dos Santos', 'do Nascimento', 'da Silva', 'de Farias', 'Vitorino', 'da Silva', 'Filho', 'Gubert']\n",
      "Abstract: Alzheimer's disease (AD) is the most common form of dementia, posing significant challenges to cognitive, emotional, social, and financial well-being. The biochemical and molecular pathways associated...\n",
      "\n",
      "PMID: 40653482\n",
      "Title: Multi-omics analysis for identifying cell-type-specific and bulk-level druggable targets in Alzheimer's disease.\n",
      "Year: 2025\n",
      "Journal: Journal of translational medicine\n",
      "Authors: ['Liu', 'Cho', 'Huang', 'Park', 'Chaudhuri', 'Rosewood', 'Bice', 'Chung', 'Bennett', 'Ertekin-Taner', 'Saykin', 'Nho']\n",
      "Abstract: Analyzing disease-linked genetic variants via expression quantitative trait loci (eQTLs) helps identify potential disease-causing genes. Previous research prioritized genes by integrating Genome-Wide ...\n",
      "\n",
      "PMID: 40634782\n",
      "Title: Plasma proteomics links brain and immune system aging with healthspan and longevity.\n",
      "Year: 2025\n",
      "Journal: Nature medicine\n",
      "Authors: ['Oh', 'Le Guen', 'Rappoport', 'Urey', 'Farinas', 'Rutledge', 'Channappa', 'Wagner', 'Mormino', 'Brunet', 'Greicius', 'Wyss-Coray']\n",
      "Abstract: Plasma proteins derived from specific organs can estimate organ age and mortality, but their sensitivity to environmental factors and their robustness in forecasting onset of organ diseases and mortal...\n",
      "\n",
      "PMID: 40616179\n",
      "Title: Machine-learning based strategy identifies a robust protein biomarker panel for Alzheimer's disease in cerebrospinal fluid.\n",
      "Year: 2025\n",
      "Journal: Alzheimer's research & therapy\n",
      "Authors: ['Hou', 'Qiu', 'Li', 'Yan', 'Zhao', 'Ji', 'Ni', 'Zhang', 'Liu', 'Qing', 'Quan']\n",
      "Abstract: The complex pathogenesis of Alzheimer's disease (AD) has resulted in limited current biomarkers for its classification and diagnosis, necessitating further investigation into reliable universal biomar...\n",
      "\n",
      "PMID: 40613333\n",
      "Title: Unravelling the plasma proteome: Pioneering biomarkers for differential dementia diagnosis.\n",
      "Year: 2025\n",
      "Journal: Alzheimer's & dementia : the journal of the Alzheimer's Association\n",
      "Authors: ['Gezegen', 'Alaylıoğlu', 'Şahin', 'Swann', 'Veleva', 'Güven', 'Yaman', 'Salih', 'Bilgiç', 'Hanağası', 'Gürvit', 'Emre', 'Gezen-Ak', 'Dursun', 'Zetterberg', 'Hardy', 'Heslegrave', 'Shoai', 'Samanci']\n",
      "Abstract: Diagnosing Alzheimer's disease (AD) is challenging due to overlapping symptoms with other dementias and the invasiveness of current biomarkers. This study introduces the NULISA platform, a novel prote...\n",
      "\n",
      "PMID: 40604345\n",
      "Title: Gut microbiome compositional and functional features associate with Alzheimer's disease pathology.\n",
      "Year: 2025\n",
      "Journal: Alzheimer's & dementia : the journal of the Alzheimer's Association\n",
      "Authors: ['Kang', 'Khatib', 'Heston', 'Dilmore', 'Labus', 'Deming', 'Schimmel', 'Blach', 'McDonald', 'Gonzalez', 'Bryant', 'Ulland', 'Johnson', 'Asthana', 'Carlsson', 'Chin', 'Blennow', 'Zetterberg', 'Rey', None, 'Kaddurah-Daouk', 'Knight', 'Bendlin']\n",
      "Abstract: The gut microbiome is a potentially modifiable risk factor for Alzheimer's disease (AD); however, understanding of its composition and function regarding AD pathology is limited....\n",
      "\n",
      "PMID: 40600356\n",
      "Title: Milestone Review: The History of Molecular Genetics Analysis of Alzheimer's Disease.\n",
      "Year: 2025\n",
      "Journal: Journal of neurochemistry\n",
      "Authors: ['Hardy']\n",
      "Abstract: Alzheimer research has been driven by genetic findings: from the 1990s until about 2005, by the identification of amyloid precursor protein (APP) and presenilin (PSEN) mutations, leading to the formul...\n",
      "\n",
      "PMID: 40595720\n",
      "Title: Proteomic analysis of Down syndrome cerebrospinal fluid compared to late-onset and autosomal dominant Alzheimer´s disease.\n",
      "Year: 2025\n",
      "Journal: Nature communications\n",
      "Authors: ['Montoliu-Gaya', 'Bian', 'Dammer', 'Alcolea', 'Sauer', 'Martá-Ariza', 'Ashton', 'Belbin', 'Fuchs', 'Watson', 'Ping', 'Duong', 'Nilsson', 'Barroeta', 'Lantero-Rodriguez', 'Videla', 'Benejam', 'Roberts', 'Blennow', 'Seyfried', 'Levey', 'Carmona-Iragui', 'Gobom', 'Lleó', 'Wisniewski', 'Zetterberg', 'Fortea', 'Johnson']\n",
      "Abstract: Almost all individuals with Down Syndrome (DS) develop Alzheimer's disease (AD) by mid to late life. However, the degree to which AD in DS shares pathological changes with sporadic late-onset AD (LOAD...\n",
      "\n",
      "PMID: 40595564\n",
      "Title: Enrichment of extracellular vesicles using Mag-Net for the analysis of the plasma proteome.\n",
      "Year: 2025\n",
      "Journal: Nature communications\n",
      "Authors: ['Wu', 'Tsantilas', 'Park', 'Plubell', 'Sanders', 'Naicker', 'Govender', 'Buthelezi', 'Stoychev', 'Jordaan', 'Merrihew', 'Huang', 'Parker', 'Riffle', 'Hoofnagle', 'Noble', 'Poston', 'Montine', 'MacCoss']\n",
      "Abstract: Extracellular vesicles (EVs) in plasma are composed of exosomes, microvesicles, and apoptotic bodies. We report a plasma EV enrichment strategy using magnetic beads called Mag-Net. Proteomic interroga...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fetch metadata for the first 20 PMIDs as a sample\n",
    "sample_pmids = all_pmids[:20]\n",
    "\n",
    "async def fetch_sample_metadata():\n",
    "    metadata = await pubmed_service.fetch_article_metadata(sample_pmids)\n",
    "    print(f\"Fetched metadata for {len(metadata)} articles.\")\n",
    "    for article in metadata:\n",
    "        print(f\"PMID: {article.pmid}\")\n",
    "        print(f\"Title: {article.title}\")\n",
    "        print(f\"Year: {article.year}\")\n",
    "        print(f\"Journal: {article.journal}\")\n",
    "        print(f\"Authors: {[a.last for a in article.authors]}\")\n",
    "        print(f\"Abstract: {article.abstract[:200] if article.abstract else ''}...\\n\")\n",
    "    return metadata\n",
    "\n",
    "sample_metadata = await fetch_sample_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bd0109",
   "metadata": {},
   "source": [
    "## 5. Filter Articles by Keywords\n",
    "\n",
    "Filter the fetched articles by keywords in the title or abstract (e.g., 'amyloid', 'tau')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07a7e5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles matching keywords ['amyloid', 'tau']: 7\n",
      "PMID: 40810263 | Title: Plasma proteomic analysis identifies proteins and pathways related to Alzheimer's risk.\n",
      "PMID: 40696469 | Title: CHIT1 and DDAH1 levels relate to amyloid-related imaging abnormalities risk profile in Alzheimer's disease patients.\n",
      "PMID: 40665049 | Title: APOE ε4 carriers share immune-related proteomic changes across neurodegenerative diseases.\n",
      "PMID: 40660303 | Title: Proteomic landscape of Alzheimer's disease: emerging technologies, advances and insights (2021 - 2025).\n",
      "PMID: 40653809 | Title: Label-Free Proteomic Profiling of the dvls2 (CL2006) Caenorhabditis elegans Alzheimer's Disease (AD) Model Reveals Conserved Molecular Signatures Shared With the Human AD Brain.\n",
      "PMID: 40600356 | Title: Milestone Review: The History of Molecular Genetics Analysis of Alzheimer's Disease.\n",
      "PMID: 40595720 | Title: Proteomic analysis of Down syndrome cerebrospinal fluid compared to late-onset and autosomal dominant Alzheimer´s disease.\n"
     ]
    }
   ],
   "source": [
    "# Filter articles by keywords in title or abstract\n",
    "keywords = [\"amyloid\", \"tau\"]\n",
    "filtered_articles = PubMedQueryService.filter_articles_by_keywords(sample_metadata, keywords)\n",
    "\n",
    "print(f\"Articles matching keywords {keywords}: {len(filtered_articles)}\")\n",
    "for article in filtered_articles:\n",
    "    print(f\"PMID: {article.pmid} | Title: {article.title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383d315e",
   "metadata": {},
   "source": [
    "## 6. Summarize and Plot Article Metadata\n",
    "\n",
    "Use PubMedAnalyzer to summarize the distribution of publication years, journals, and MeSH terms, and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03af7304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize and plot article metadata using PubMedAnalyzer\n",
    "from niagads.pubmed.parsers import PubMedAnalyzer\n",
    "analyzer = PubMedAnalyzer(sample_metadata)\n",
    "analyzer.summarize()\n",
    "print(analyzer.summary)\n",
    "analyzer.plot_summary()\n",
    "print(analyzer.summary.mesh_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398664e8",
   "metadata": {},
   "source": [
    "## 7. Generate a Word Cloud from Article Titles\n",
    "\n",
    "This step visualizes the most frequent and semantically meaningful phrases in article titles using the `TextSummarizer` class. The `TextSummarizer` leverages large language models (LLMs) for phrase extraction and, optionally, semantic clustering. When enabled, LLM-based embeddings group similar phrases together, allowing the word cloud to reflect not just raw frequency but also semantic similarity between phrases. This provides a more insightful view of the key topics and concepts present in the article titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06daa97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-pubmed and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\n requires the protobuf library but it was not found in your environment. Check out the instructions on the\ninstallation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ai/niagads-pylib/.venv/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:1737\u001b[39m, in \u001b[36mconvert_slow_tokenizer\u001b[39m\u001b[34m(transformer_tokenizer, from_tiktoken)\u001b[39m\n\u001b[32m   1733\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mConverting from Tiktoken\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTikTokenConverter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1735\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformer_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1736\u001b[39m \u001b[43m        \u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformer_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m1737\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ai/niagads-pylib/.venv/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:1631\u001b[39m, in \u001b[36mTikTokenConverter.converted\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1630\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconverted\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Tokenizer:\n\u001b[32m-> \u001b[39m\u001b[32m1631\u001b[39m     tokenizer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1632\u001b[39m     tokenizer.pre_tokenizer = pre_tokenizers.Sequence(\n\u001b[32m   1633\u001b[39m         [\n\u001b[32m   1634\u001b[39m             pre_tokenizers.Split(Regex(\u001b[38;5;28mself\u001b[39m.pattern), behavior=\u001b[33m\"\u001b[39m\u001b[33misolated\u001b[39m\u001b[33m\"\u001b[39m, invert=\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m   1635\u001b[39m             pre_tokenizers.ByteLevel(add_prefix_space=\u001b[38;5;28mself\u001b[39m.add_prefix_space, use_regex=\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m   1636\u001b[39m         ]\n\u001b[32m   1637\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ai/niagads-pylib/.venv/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:1624\u001b[39m, in \u001b[36mTikTokenConverter.tokenizer\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1623\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtokenizer\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1624\u001b[39m     vocab_scores, merges = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mextract_vocab_merges_from_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1625\u001b[39m     tokenizer = Tokenizer(BPE(vocab_scores, merges, fuse_unk=\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ai/niagads-pylib/.venv/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:1600\u001b[39m, in \u001b[36mTikTokenConverter.extract_vocab_merges_from_model\u001b[39m\u001b[34m(self, tiktoken_url)\u001b[39m\n\u001b[32m   1596\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1597\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`tiktoken` is required to read a `tiktoken` file. Install it with `pip install tiktoken`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1598\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1600\u001b[39m bpe_ranks = \u001b[43mload_tiktoken_bpe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtiktoken_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1601\u001b[39m byte_encoder = bytes_to_unicode()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ai/niagads-pylib/.venv/lib/python3.12/site-packages/tiktoken/load.py:158\u001b[39m, in \u001b[36mload_tiktoken_bpe\u001b[39m\u001b[34m(tiktoken_bpe_file, expected_hash)\u001b[39m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_tiktoken_bpe\u001b[39m(tiktoken_bpe_file: \u001b[38;5;28mstr\u001b[39m, expected_hash: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[32m    157\u001b[39m     \u001b[38;5;66;03m# NB: do not add caching to this function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     contents = \u001b[43mread_file_cached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtiktoken_bpe_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_hash\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m     ret = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ai/niagads-pylib/.venv/lib/python3.12/site-packages/tiktoken/load.py:48\u001b[39m, in \u001b[36mread_file_cached\u001b[39m\u001b[34m(blobpath, expected_hash)\u001b[39m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m read_file(blobpath)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m cache_key = hashlib.sha1(\u001b[43mblobpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m()).hexdigest()\n\u001b[32m     50\u001b[39m cache_path = os.path.join(cache_dir, cache_key)\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'encode'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ai/niagads-pylib/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2316\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2315\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2316\u001b[39m     tokenizer = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2317\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ai/niagads-pylib/.venv/lib/python3.12/site-packages/transformers/models/pegasus/tokenization_pegasus_fast.py:136\u001b[39m, in \u001b[36mPegasusTokenizerFast.__init__\u001b[39m\u001b[34m(self, vocab_file, tokenizer_file, pad_token, eos_token, unk_token, mask_token, mask_token_sent, additional_special_tokens, offset, **kwargs)\u001b[39m\n\u001b[32m    134\u001b[39m kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33madded_tokens_decoder\u001b[39m\u001b[33m\"\u001b[39m, {})\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43munk_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask_token_sent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask_token_sent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m    \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m    \u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_slow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_slow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[38;5;28mself\u001b[39m.vocab_file = vocab_file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ai/niagads-pylib/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:139\u001b[39m, in \u001b[36mPreTrainedTokenizerFast.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    138\u001b[39m \u001b[38;5;28mself\u001b[39m.additional_special_tokens = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33madditional_special_tokens\u001b[39m\u001b[33m\"\u001b[39m, [])\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m fast_tokenizer = \u001b[43mconvert_slow_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tiktoken\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m slow_tokenizer = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ai/niagads-pylib/.venv/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:1739\u001b[39m, in \u001b[36mconvert_slow_tokenizer\u001b[39m\u001b[34m(transformer_tokenizer, from_tiktoken)\u001b[39m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1740\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConverting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1741\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mwith a SentencePiece tokenizer.model file.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1742\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCurrently available slow->fast converters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(SLOW_TO_FAST_CONVERTERS.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1743\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m     titles = []\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m titles:\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     summarizer = \u001b[43mTextSummarizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     clusters = summarizer.semantic_phrase_clustering(titles, top_n=\u001b[32m30\u001b[39m, min_ngram=\u001b[32m1\u001b[39m, max_ngram=\u001b[32m3\u001b[39m, use_embeddings=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     15\u001b[39m     summarizer.plot_ngram_wordcloud(clusters, max_words=\u001b[32m50\u001b[39m, title=\u001b[33m\"\u001b[39m\u001b[33mWord Cloud of Article Titles\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ai/niagads-pylib/components/niagads/nlp_utils/summarizer.py:92\u001b[39m, in \u001b[36mTextSummarizer.__init__\u001b[39m\u001b[34m(self, model, max_length, min_length)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28mself\u001b[39m.__max_length = max_length\n\u001b[32m     91\u001b[39m \u001b[38;5;28mself\u001b[39m.__min_length = min_length\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m \u001b[38;5;28mself\u001b[39m.__summarizer = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msummarization\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__model\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ai/niagads-pylib/.venv/lib/python3.12/site-packages/transformers/pipelines/__init__.py:1079\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[39m\n\u001b[32m   1077\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1078\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m load_tokenizer:\n\u001b[32m-> \u001b[39m\u001b[32m1079\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1080\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1081\u001b[39m         tokenizer = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ai/niagads-pylib/.venv/lib/python3.12/site-packages/transformers/pipelines/__init__.py:1074\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[39m\n\u001b[32m   1071\u001b[39m             tokenizer_kwargs = model_kwargs.copy()\n\u001b[32m   1072\u001b[39m             tokenizer_kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mtorch_dtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m), tokenizer_kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1074\u001b[39m         tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1075\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtokenizer_identifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fast\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_fast\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1078\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m load_tokenizer:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ai/niagads-pylib/.venv/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:1144\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1141\u001b[39m tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[32m   1143\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m1144\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class_fast\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1146\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ai/niagads-pylib/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2070\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2067\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2068\u001b[39m         logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2070\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2073\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2074\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2076\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2077\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2078\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2079\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2080\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2081\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2082\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ai/niagads-pylib/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2317\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2315\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   2316\u001b[39m     tokenizer = \u001b[38;5;28mcls\u001b[39m(*init_inputs, **init_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m2317\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[43mimport_protobuf_decode_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   2318\u001b[39m     logger.info(\n\u001b[32m   2319\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2320\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2321\u001b[39m     )\n\u001b[32m   2322\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ai/niagads-pylib/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:87\u001b[39m, in \u001b[36mimport_protobuf_decode_error\u001b[39m\u001b[34m(error_message)\u001b[39m\n\u001b[32m     85\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DecodeError\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(PROTOBUF_IMPORT_ERROR.format(error_message))\n",
      "\u001b[31mImportError\u001b[39m: \n requires the protobuf library but it was not found in your environment. Check out the instructions on the\ninstallation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "# Generate a word cloud from article titles using TextSummarizer\n",
    "from niagads.nlp_utils.summarizer import TextSummarizer\n",
    "\n",
    "# Collect all article titles (use filtered_articles if available, else sample_metadata)\n",
    "if 'filtered_articles' in locals() and filtered_articles:\n",
    "    titles = [article.title for article in filtered_articles if article.title]\n",
    "elif 'sample_metadata' in locals() and sample_metadata:\n",
    "    titles = [article.title for article in sample_metadata if article.title]\n",
    "else:\n",
    "    titles = []\n",
    "\n",
    "if titles:\n",
    "    summarizer = TextSummarizer()\n",
    "    clusters = summarizer.semantic_phrase_clustering(titles, top_n=30, min_ngram=1, max_ngram=3, use_embeddings=False)\n",
    "    summarizer.plot_ngram_wordcloud(clusters, max_words=50, title=\"Word Cloud of Article Titles\")\n",
    "else:\n",
    "    print(\"No article titles available for word cloud visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3631c79",
   "metadata": {},
   "source": [
    "## 8. Fetch Full Text for Selected PMIDs\n",
    "\n",
    "Fetch full text XML for selected PMIDs from the PMC Open Access subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2cb77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch full text for the first filtered article (if available)\n",
    "if filtered_articles:\n",
    "    pmid = filtered_articles[0].pmid\n",
    "    full_text_results = await pubmed_service.fetch_full_text([pmid])\n",
    "  \n",
    "else:\n",
    "    full_text_xml = None\n",
    "    print(\"No filtered articles available for full text fetch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9bb6ba",
   "metadata": {},
   "source": [
    "## 9. Parse PMC Full Text XML\n",
    "\n",
    "Create a PMCFullTextParser instance with the full text XML and parse it to extract content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abd9b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the full text XML if available\n",
    "if full_text_xml:\n",
    "    parser = PMCFullTextParser(full_text_xml)\n",
    "    parser.parse()\n",
    "    print(\"PMC full text parsed.\")\n",
    "else:\n",
    "    parser = None\n",
    "    print(\"No full text XML available to parse.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064a7fb3",
   "metadata": {},
   "source": [
    "## 10. Extract Tables, Sections, and Figures from Full Text\n",
    "\n",
    "Use PMCFullTextParser methods to extract tables, sections, and figures from the parsed XML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b32eaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and display tables, sections, and figures from the parsed PMC full text\n",
    "if parser:\n",
    "    tables = parser.get_tables()\n",
    "    sections = parser.get_sections()\n",
    "    figures = parser.get_figures()\n",
    "    print(f\"Tables found: {len(tables)}\")\n",
    "    print(f\"Sections found: {len(sections)}\")\n",
    "    print(f\"Figures found: {len(figures)}\")\n",
    "    if tables:\n",
    "        print(\"First table:\", tables[0])\n",
    "    if sections:\n",
    "        print(\"First section:\", sections[0])\n",
    "    if figures:\n",
    "        print(\"First figure:\", figures[0])\n",
    "else:\n",
    "    print(\"No parsed PMC full text to extract content from.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c11a991",
   "metadata": {},
   "source": [
    "## 11. Extract Specific Sections: Results, Conclusions, Discussion\n",
    "\n",
    "Extract and display only the sections with titles containing 'Results', 'Conclusions', or 'Discussion' from the parsed sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbbcc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and display 'Results', 'Conclusions', 'Discussion' sections using LLM-based synonym expansion\n",
    "from niagads.nlp_utils.matchers.synonyms import LLMSynonymMatcher, SynonymModel\n",
    "import asyncio\n",
    "if parser:\n",
    "    # Define section titles to extract (case-insensitive)\n",
    "    section_titles = [\"Results\", \"Conclusions\", \"Discussion\"]\n",
    "    # Use LLMSynonymMatcher to get extended list of synonyms for section titles\n",
    "    matcher = LLMSynonymMatcher(section_titles, model=SynonymModel.T5)\n",
    "    extended_synonyms = asyncio.run(matcher.get_extended_synonyms())\n",
    "    print(f\"Section titles + LLM synonyms: {extended_synonyms}\")\n",
    "    # Use parser.get_sections with the extended_synonyms as the titles argument\n",
    "    filtered_sections = parser.get_sections(titles=list(extended_synonyms))\n",
    "    print(f\"Sections matching {section_titles} (with LLM synonyms): {len(filtered_sections)}\")\n",
    "    for sec in filtered_sections:\n",
    "        print(f\"Title: {sec['title']}\")\n",
    "        print(f\"Text: {sec['text'][:500]}...\\n\")\n",
    "else:\n",
    "    print(\"No parsed PMC full text to extract specific sections from.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc9729e",
   "metadata": {},
   "source": [
    "## 12. Extract Gene References from Filtered Sections\n",
    "\n",
    "Use the LLM-based gene extractor to identify gene mentions in the filtered sections. This step demonstrates how to apply the `GeneReferenceExtractor` to biomedical text extracted from PMC full text articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f646542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract gene references from the filtered sections using GeneReferenceExtractor\n",
    "from niagads.nlp_utils.extractors.gene import GeneReferenceExtractor, GeneNERModel\n",
    "\n",
    "gene_extractor = GeneReferenceExtractor(model=GeneNERModel.D4DATA)\n",
    "\n",
    "# Combine all filtered section texts into one block for extraction\n",
    "if parser and 'filtered_sections' in locals():\n",
    "    all_section_text = \"\\n\".join(sec['text'] for sec in filtered_sections)\n",
    "    gene_mentions = gene_extractor.extract(all_section_text)\n",
    "    print(f\"Gene mentions found: {gene_mentions}\")\n",
    "else:\n",
    "    print(\"No filtered sections available for gene extraction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a289bb",
   "metadata": {},
   "source": [
    "## 13. Summarize Gene Contexts in Filtered Sections\n",
    "\n",
    "Use the `TextSummarizer` to generate a summary for each gene, based on the sentences in which it appears in the filtered sections. This provides a concise overview of the context in which each gene is discussed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e224293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize gene contexts using TextSummarizer\n",
    "from niagads.nlp_utils.summarizer import TextSummarizer, SummarizationModel\n",
    "\n",
    "# Extract gene context sentences (per gene) from the filtered sections\n",
    "if parser and 'filtered_sections' in locals():\n",
    "    all_section_text = \"\\n\".join(sec['text'] for sec in filtered_sections)\n",
    "    gene_contexts = gene_extractor.extract_gene_contexts(all_section_text)\n",
    "    summarizer = TextSummarizer(model=SummarizationModel.PEGASUS_PUBMED)\n",
    "    gene_summaries = summarizer.summarize(gene_contexts)\n",
    "    for gene, summary in gene_summaries.items():\n",
    "        print(f\"Gene: {gene}\\nSummary: {summary}\\n\")\n",
    "else:\n",
    "    print(\"No filtered sections available for gene context summarization.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
