# Ensembl GFF3 parser that transforms nested gene structure into a gene model document
# code adapted from solution generated by ChatGPT 5.0, 
# see chat log: https://chatgpt.com/share/689f9cb0-6c14-8004-96da-195cbb0e7444

from __future__ import annotations

import gzip
import io
import json
from collections import defaultdict
from typing import Dict, Any, Optional, Set, List, Generator, Union

# ------------------------------------------------------------
# Helpers
# ------------------------------------------------------------
def _open_any(path: str) -> io.TextIOBase:
    if path.endswith(".gz"):
        return io.TextIOWrapper(gzip.open(path, "rb"), encoding="utf-8")
    return open(path, "r", encoding="utf-8")

def _norm_seqid(seqid: str, *, normalize_chroms: bool, add_chr_prefix: bool) -> str:
    if not normalize_chroms:
        return seqid
    sid = seqid
    if add_chr_prefix and not sid.startswith("chr"):
        sid = f"chr{sid}"
    if sid == "chrMT":
        sid = "chrM"
    return sid

def _strip_prefix(val: Optional[str]) -> Optional[str]:
    if val is None:
        return None
    return val.split(":")[-1]

_FTYPE_NORMALIZE_MAP = {
    "five_prime_utr": "five_prime_utr",
    "three_prime_utr": "three_prime_utr",
    "lncrna": "lnc_rna",
    "lnc_rna": "lnc_rna",
    "mirna": "mirna",
    "snrna": "snrna",
    "snorna": "snorna",
    "rrna": "rrna",
    "ncrna": "ncrna",
}

def _normalize_ftype(ftype: str) -> str:
    s = ftype.lower()
    # Normalize common Ensembl spellings/cases
    if s == "five_prime_utr" or s == "five_prime_utr".lower():
        return "five_prime_utr"
    if s == "three_prime_utr" or s == "three_prime_utr".lower():
        return "three_prime_utr"
    # Handle UTR tokens written as "five_prime_utr"/"three_prime_utr" or "five_prime_utr"/"three_prime_utr"
    s = s.replace("five_prime_utr", "five_prime_utr").replace("three_prime_utr", "three_prime_utr")
    # lncRNA variants / small RNAs
    s = s.replace("lncrna", "lnc_rna").replace("lnc_rna", "lnc_rna")
    s = s.replace("mirna", "mirna").replace("snrna", "snrna").replace("snorna", "snorna")
    s = s.replace("rrna", "rrna").replace("ncrna", "ncrna")
    return s

def _parse_attrs_for_ids(attr_field: str) -> Dict[str, Any]:
    """
    Minimal attribute parser: only captures ID and Parent (prefix-stripped).
    """
    out: Dict[str, Any] = {}
    if not attr_field or attr_field == ".":
        return out
    for item in attr_field.split(";"):
        if not item:
            continue
        if "=" in item:
            k, v = item.split("=", 1)
            if k in ("ID", "Parent"):
                v0 = v.split(",")[0] if v else ""
                out[k] = _strip_prefix(v0)
        else:
            # ignore bare flags for this use-case
            pass
    return out

def _strand_symbol(s: str) -> str:
    return "+" if s == "+" else "-" if s == "-" else "."

def _drop_internal(n: Dict[str, Any]) -> Dict[str, Any]:
    return {k: v for k, v in n.items() if not k.startswith("_")}

def _maybe_drop_children(n: Dict[str, Any]) -> Dict[str, Any]:
    if "children" in n and (not n["children"]):
        n.pop("children", None)
    return n

# ------------------------------------------------------------
# Core parse + link (shared by both APIs)
# ------------------------------------------------------------
def _parse_and_link_gff3(
    gff_path: str,
    *,
    normalize_chroms: bool = True,
    add_chr_prefix: bool = True,
) -> Dict[str, Any]:
    """
    Single-pass read; second-pass linking by ID/Parent.
    Returns dict with 'by_id', 'roots_by_chrom' for downstream shaping.
    Nodes are already normalized per your revised rules:
      - prefix-stripped 'id' and '_parent_id'
      - lowercase/snake_case 'type'
      - 'source' lowercased (or None)
      - 'location' with start, end, strand
      - 'parent' filled in on children
      - 'children' omitted if empty (after linking)
    """
    by_id: Dict[str, Dict[str, Any]] = {}
    roots_by_chrom: Dict[str, List[Dict[str, Any]]] = defaultdict(list)
    pending_children: Dict[str, List[str]] = defaultdict(list)
    anonymous_nodes_by_chrom: Dict[str, List[Dict[str, Any]]] = defaultdict(list)

    with _open_any(gff_path) as fh:
        for raw_line in fh:
            if not raw_line or raw_line.startswith("#"):
                continue
            parts = raw_line.rstrip("\n").split("\t")
            if len(parts) < 9:
                continue

            seqid, source, ftype, start, end, score, strand, phase, attrs = parts
            chrom = _norm_seqid(seqid, normalize_chroms=normalize_chroms, add_chr_prefix=add_chr_prefix)
            ftype_norm = _normalize_ftype(ftype)

            start_i, end_i = int(start), int(end)
            a = _parse_attrs_for_ids(attrs)
            fid = _strip_prefix(a.get("ID"))
            parent = _strip_prefix(a.get("Parent"))

            node = {
                "id": fid,
                "type": ftype_norm,
                "source": None if source == "." else source.lower(),
                "location": {"start": start_i, "end": end_i, "strand": _strand_symbol(strand)},
                "children": [],     # will drop if empty
                "parent": None,     # will be set when linked to a parent
                "_parent_id": parent,
                "_seqid": chrom,
            }

            if fid:
                by_id[fid] = node
            else:
                anonymous_nodes_by_chrom[chrom].append(node)

            if parent and fid:
                pending_children[parent].append(fid)

    # Link children; set compact parent
    for parent_id, child_ids in pending_children.items():
        parent_node = by_id.get(parent_id)
        if not parent_node:
            continue
        for cid in child_ids:
            ch = by_id.get(cid)
            if ch:
                parent_node["children"].append(ch)
                ch["parent"] = {"parent_id": parent_id, "parent_type": parent_node["type"]}

    # Roots: without valid parent
    for fid, node in by_id.items():
        if not node.get("_parent_id") or node["_parent_id"] not in by_id:
            roots_by_chrom[node["_seqid"]].append(node)
    for chrom, nodes in anonymous_nodes_by_chrom.items():
        roots_by_chrom[chrom].extend(nodes)

    # Drop empty children arrays
    for n in by_id.values():
        _maybe_drop_children(n)
    for roots in roots_by_chrom.values():
        for n in roots:
            _maybe_drop_children(n)

    return {"by_id": by_id, "roots_by_chrom": roots_by_chrom}

# ------------------------------------------------------------
# Public API 1: original nested output (gene → transcripts → subfeatures)
# ------------------------------------------------------------
def gff3_to_gene_nested_json(
    gff_path: str,
    *,
    normalize_chroms: bool = True,
    add_chr_prefix: bool = True,
    transcript_like: Optional[Set[str]] = None,
    subfeature_like: Optional[Set[str]] = None,
) -> Dict[str, Any]:
    """
    Return the nested, cleaned JSON:
      { "assembly": "grch38", "source": "ensembl", "records": { "chrN": [ gene, ... ] } }
    """
    if transcript_like is None:
        transcript_like = {"transcript"}  # per your latest guidance
    if subfeature_like is None:
        subfeature_like = {"exon", "cds", "five_prime_utr", "three_prime_utr"}

    parsed = _parse_and_link_gff3(gff_path, normalize_chroms=normalize_chroms, add_chr_prefix=add_chr_prefix)
    roots_by_chrom: Dict[str, List[Dict[str, Any]]] = parsed["roots_by_chrom"]

    def _sorted_children(children: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        return sorted(children, key=lambda n: (n["location"]["start"], n["location"]["end"], n.get("type", ""), n.get("id") or ""))

    def _shape_gene_tree(root: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        if root["type"] != "gene":
            return None

        # Partition gene children
        ts, others = [], []
        for ch in root.get("children", []) or []:
            (ts if ch.get("type") in transcript_like else others).append(ch)

        new_ts: List[Dict[str, Any]] = []
        for t in ts:
            sub, sub_others = [], []
            for ch in t.get("children", []) or []:
                (sub if ch.get("type") in subfeature_like else sub_others).append(ch)

            t_new = _drop_internal(t)
            shaped_children = _sorted_children([_drop_internal(x) for x in sub] + [_drop_internal(x) for x in sub_others])
            if shaped_children:
                t_new["children"] = shaped_children
            _maybe_drop_children(t_new)
            new_ts.append(t_new)

        g_new = _drop_internal(root)
        g_children = _sorted_children(new_ts + [_drop_internal(x) for x in others])
        if g_children:
            g_new["children"] = g_children
        _maybe_drop_children(g_new)
        return g_new

    records: Dict[str, List[Dict[str, Any]]] = {}
    for chrom, roots in roots_by_chrom.items():
        gene_trees = []
        for r in roots:
            shaped = _shape_gene_tree(r)
            if shaped is not None:
                gene_trees.append(shaped)
        if gene_trees:
            gene_trees.sort(key=lambda g: (g["location"]["start"], g["location"]["end"]))
            records[chrom] = gene_trees

    return {"assembly": "grch38", "source": "ensembl", "records": records}

# ------------------------------------------------------------
# Public API 2: NEW — per-gene documents for a document DB
# ------------------------------------------------------------
def gff3_to_gene_documents(
    gff_path: str,
    *,
    normalize_chroms: bool = True,
    add_chr_prefix: bool = True,
    include_transcripts_by_id: bool = False,
) -> Generator[Dict[str, Any], None, None]:
    """
    Yield one denormalized gene document per gene, ready for a document DB.

    Gene document shape (only present arrays are emitted):
      {
        "_id": "ENSG...",
        "gene_id": "ENSG...",
        "chromosome": "chr1",
        "start": <int>, "end": <int>, "strand": "+|-" ,
        "source": "ensembl_havana" | null,
        "transcripts": [ {id, start, end, strand, source}, ... ],
        "exons": [ {id, transcript_id, start, end, strand, source}, ... ],
        "cds":   [ {id, transcript_id, start, end, strand, source}, ... ],
        "five_prime_utrs":  [ { ... } ],
        "three_prime_utrs": [ { ... } ],
        "other_features": { "<type>": [ { id, (transcript_id?), start, end, strand, source }, ... ] }
      }
    """
    parsed = _parse_and_link_gff3(gff_path, normalize_chroms=normalize_chroms, add_chr_prefix=add_chr_prefix)
    roots_by_chrom: Dict[str, List[Dict[str, Any]]] = parsed["roots_by_chrom"]

    HANDLED_TYPES = {"exon", "cds", "five_prime_utr", "three_prime_utr"}

    def _loc(n: Dict[str, Any]) -> Dict[str, Any]:
        loc = n.get("location", {}) or {}
        return {"start": loc.get("start"), "end": loc.get("end"), "strand": loc.get("strand")}

    for chrom, roots in roots_by_chrom.items():
        for gene in roots:
            if gene.get("type") != "gene" or not gene.get("id"):
                continue

            gdoc: Dict[str, Any] = {
                "_id": gene["id"],
                "gene_id": gene["id"],
                "chromosome": chrom,
                "start": gene["location"]["start"],
                "end": gene["location"]["end"],
                "strand": gene["location"]["strand"],
                "source": gene.get("source"),
            }

            transcripts: List[Dict[str, Any]] = []
            transcripts_by_id: Dict[str, Dict[str, Any]] = {}
            exons: List[Dict[str, Any]] = []
            cds_list: List[Dict[str, Any]] = []
            utr5: List[Dict[str, Any]] = []
            utr3: List[Dict[str, Any]] = []
            other: Dict[str, List[Dict[str, Any]]] = defaultdict(list)

            for t in gene.get("children", []) or []:
                if t.get("type") == "transcript":
                    t_min = {"id": t.get("id"), **_loc(t), "source": t.get("source")}
                    transcripts.append(t_min)
                    if include_transcripts_by_id and t_min["id"]:
                        transcripts_by_id[t_min["id"]] = t_min
                    # Fan out subfeatures
                    for ch in t.get("children", []) or []:
                        entry = {"id": ch.get("id"), "transcript_id": t_min["id"], **_loc(ch), "source": ch.get("source")}
                        ctype = ch.get("type")
                        if ctype == "exon":
                            exons.append(entry)
                        elif ctype == "cds":
                            cds_list.append(entry)
                        elif ctype == "five_prime_utr":
                            utr5.append(entry)
                        elif ctype == "three_prime_utr":
                            utr3.append(entry)
                        else:
                            if ctype:
                                other[ctype].append(entry)
                else:
                    # Rare: non-transcript children directly under gene
                    ft = t.get("type")
                    if ft:
                        other[ft].append({"id": t.get("id"), **_loc(t), "source": t.get("source")})

            # Sort for stability
            transcripts.sort(key=lambda x: (x.get("start", 0), x.get("end", 0), x.get("id") or ""))
            exons.sort(key=lambda x: (x.get("start", 0), x.get("end", 0), x.get("transcript_id") or "", x.get("id") or ""))
            cds_list.sort(key=lambda x: (x.get("start", 0), x.get("end", 0), x.get("transcript_id") or "", x.get("id") or ""))
            utr5.sort(key=lambda x: (x.get("start", 0), x.get("end", 0), x.get("transcript_id") or "", x.get("id") or ""))
            utr3.sort(key=lambda x: (x.get("start", 0), x.get("end", 0), x.get("transcript_id") or "", x.get("id") or ""))

            # Assemble only non-empty fields
            if transcripts:
                gdoc["transcripts"] = transcripts
            if include_transcripts_by_id and transcripts_by_id:
                gdoc["transcripts_by_id"] = transcripts_by_id
            if exons:
                gdoc["exons"] = exons
            if cds_list:
                gdoc["cds"] = cds_list
            if utr5:
                gdoc["five_prime_utrs"] = utr5
            if utr3:
                gdoc["three_prime_utrs"] = utr3
            if other:
                gdoc["other_features"] = dict(other)

            yield gdoc

